gpu: cuda:0                                    # training GPU e.g cuda:0
runseed: 12                                   # seed for torch and numpy

fine_tune_dataset:
    batch_size: 128                            # batch size
    ogbg_data_path: datasets/fine_tuning/ogb  # path of fine-tuning ogbg data
    ogbg_dataset_name: sider                # name of specific ogbg dataset 'tox21','toxcast','muv','bace','bbbp','clintox','sider','esol','freesolv','lipo'

pre_train_dataset:
    batch_size: 128                            # batch size
    pubchem_data_path: datasets/pre_training/pubchem  # path of pre-training ogbg data
    pubchem_dataset_name: pubchem                # pubchem or name of specific ogbg dataset 'tox21','toxcast','muv','bace','bbbp','clintox','sider','esol','freesolv','lipo'
    train_fraction: 0.0001
    valid_fraction: 0.9999
    test_fraction: 0.0
    splitting: random                      # [scaffold, random] to determine how to split the dataset

fine_tune_model:
    num_layer: 3                              # number of message passing layers
    emb_dim: 32                               # embedding dimension
    dropout_ratio: 0.5                        # dropout ration
    graph_pooling: mean                       # graph level pooling (sum, mean, max, set2set, attention)
    JK: last                                  # how the node features across layers are combined. last, sum, max or concat
    gnn_type: gin                             # GNN type (gin, gcn)

pre_train_encoder:
    num_layer: 3
    emb_dim: 32
    dropout_ratio: 0.5                        # dropout ration
    graph_pooling: mean                       # graph level pooling (sum, mean, max, set2set, attention)
    JK: last                                  # how the node features across layers are combined. last, sum, max or concat
    gnn_type: gin                             # GNN type (gin, gcn)

fine_tune_optimizer:
    lr: 0.001                                 # learning rate
    lr_scale: 1                               # relative learning rate for the feature extraction layer
    decay: 0                                  # weight decay 

fine_tune_training:
    epoch: 100                                # total number of epochs
    eval_train: True                        # evaluating training or not
    early_stopping: False
    print_every: 10

contrastive:
    emb_dim: 32
    aug_1: uniformDropNScaffold                     # identity or maskN or maskNScaffold or uniformDropNScaffold or uniformDropN
    aug_2: null
    tau: 0.2

evaluator:
    type: node_unsupervised
    p_optim: Adam
    p_lr: 0.01
    p_weight_decay: 0
    p_epoch: 2
    log_interval: 10
    batch_size: 32
    eval_train: False                        # evaluating training or not

load_save_fine_tune:                       # when fine tuning should the model be loaded (e.g. pretrain model) and should the fine tuned model be saved
    load_model_dir: pre_training/pubchem   # directory of the model to be loaded (not including results/ e.g. ... fine_tuning/sider/
    load_model_name:   # exact file name without extension 
    save_model: True                        # True or False if want to save fine_tuning model 

load_save_pre_train:
    load_model_dir:     # directory of the model to be loaded (not including results/ e.g. ... fine_tuning/sider/
    load_model_name:    # exact file name without extension 
    save_model: True                        # True or False if want to save fine_tuning model 