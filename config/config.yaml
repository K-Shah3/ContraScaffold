gpu: cuda:0                                 # training GPU 

fine_tune_dataset:
    batch_size: 32                          # batch size
    ogbg_data_path: datasets/               # path of pre-training ogbg data
    ogbg_dataset_name: sider                # name of specific ogbg dataset

fine_tune_model:
    num_layer: 4                            # number of message passing layers
    emb_dim: 32                             # embedding dimension
    dropout_ratio: 0.5                      # dropout ration
    graph_pooling: mean                     # graph level pooling (sum, mean, max, set2set, attention)
    JK: last                                # how the node features across layers are combined. last, sum, max or concat
    gnn_type: gcn                           # GNN type (gin, gcn)

fine_tune_optimizer:
    lr: 0.001                               # learning rate
    lr_scale: 1                             # relative learning rate for the feature extraction layer
    decay: 0                                # weight decay 

fine_tune_training:
    epoch: 3                                # total number of epochs
    eval_train: True                        # evaluating training or not